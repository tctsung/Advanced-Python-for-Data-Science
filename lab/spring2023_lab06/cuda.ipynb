{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKJaR8zJbEuF"
   },
   "source": [
    "## Cuda\n",
    "\n",
    "A general purpose parallel computing platform and programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvTUtqhMbrCJ"
   },
   "source": [
    "<img align=\"right\"  src=\"https://i.ibb.co/JQGz39k/cuda1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<div style=\"text-align: left\"> \n",
    "\n",
    "    Grid: Group of blocks executing a kernel\n",
    "    One grid per CUDA kernel\n",
    "    \n",
    "    Block: Group of threads that can be scheduled independently\n",
    "    Max threads in a block: 1024\n",
    "    \n",
    "    Thread: A single context of execution\n",
    "    \n",
    "    Kernel: Function that will execute in parallel on multiple threads\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eum7mthb91D"
   },
   "source": [
    "<img src=\"https://i.ibb.co/jkyFsvc/cuda2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1AGLeZPcUk3"
   },
   "source": [
    "<div style=\"text-align: left\"> \n",
    "\n",
    "    CUDA Memory Hierarchy:\n",
    "    \n",
    "    Per thread local memory: Available only to the single thread\n",
    "    \n",
    "    Block shared memory: Shared by all the threads in a block\n",
    "        Faster than global memory\n",
    "    \n",
    "    Global memory: Shared by all blocks and all grids\n",
    "\n",
    "</div>\n",
    "\n",
    "<img align=\"left\"  src=\"https://i.ibb.co/hFp1yTc/cuda3.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpaOL8N4ccIQ"
   },
   "source": [
    "## Global memory access\n",
    "\n",
    "<img src=\"https://i.ibb.co/rQ4sLvg/matmul1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1626,
     "status": "ok",
     "timestamp": 1677734422637,
     "user": {
      "displayName": "Krishna Kartik Darsipudi",
      "userId": "03239279300658474709"
     },
     "user_tz": 300
    },
    "id": "e1CUpxF-bHYr",
    "outputId": "1e383e12-d4dc-45fd-afa4-47b2ecd7bed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jlQ-r5Qihn6"
   },
   "source": [
    "## A more complex example: matrix multiplication\n",
    "\n",
    "The following code sample is a straightforward implementation of matrix multiplication for matrices where each thread reads one row of A and one column of B and computes the corresponding element of C. For input arrays where A.shape == (m, n) and B.shape == (n, p) then the result shape will be C.shape = (m, p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1677734453092,
     "user": {
      "displayName": "Krishna Kartik Darsipudi",
      "userId": "03239279300658474709"
     },
     "user_tz": 300
    },
    "id": "5JxuII9zi57h"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    C is for saving output\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4YPVctFi99e"
   },
   "source": [
    "The host code must create and initiliaze the A and B arrays, then move them to the device. Next, it must allocate space on the device for the result array. Once the kernel has completed, the result array must be copied back to the host so that it can be displayed.\n",
    "\n",
    "Create a program called cuda2.py using the code below to see how the kernel works for the input arrays. Notice that the number of threads per block and blocks per grid is not really important, other than to ensure that there are enough threads to complete the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1021,
     "status": "ok",
     "timestamp": 1677736639781,
     "user": {
      "displayName": "Krishna Kartik Darsipudi",
      "userId": "03239279300658474709"
     },
     "user_tz": 300
    },
    "id": "1vP-iPMzjEqh",
    "outputId": "6042425c-618f-4d53-aee6-d0cca6872608",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]]\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = numpy.full((32, 48), 3, float) # matrix containing all 3's\n",
    "B = numpy.full((48, 16), 4, float) # matrix containing all 4's\n",
    "\n",
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((32, 16))    \n",
    "# > create an empty array in device for saving output\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "# > use ceil, it's better to have extra empty space\n",
    "\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "# > must include C_global_mem because CUDA cannot return values\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKoibAf_jGtx"
   },
   "source": [
    "A problem with this code is that each thread is reading from the global memory containing the copies of A and B. In fact, the A global memory is read B.shape[1] times and the B global memory is read A.shape[0] times. Since global memory is fairly slow, this results in an inefficient use of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKNfH_GOjO3p"
   },
   "source": [
    "### Shared memory and thread synchronization\n",
    "A limited amount of shared memory can be allocated on the device to speed up access to data, when necessary. That memory will be shared (i.e. both readable and writable) amongst all threads belonging to a given block and has faster access times than regular device memory. It also allows threads to cooperate on a given solution. You can think of it as a manually-managed data cache.\n",
    "\n",
    "The function to create a shared memory array is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujyprP5kjSiZ"
   },
   "source": [
    "```shared_array = cuda.shared.array(shape,type)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NShJVWo9j2mW"
   },
   "source": [
    "**shape**\n",
    "\n",
    "    is either an integer or a tuple of integers representing the array’s dimensions.\n",
    "\n",
    "**type**\n",
    "\n",
    "    is a Numba type of the elements needing to be stored in the array\n",
    "    The memory is allocated once for the duration of the kernel, unlike traditional dynamic memory management.\n",
    "\n",
    "Because the shared memory is a limited resource, it is often necessary to preload a small block at a time from the input arrays. All the threads then need to wait until everyone has finished preloading before doing the computation on the shared memory.\n",
    "\n",
    "Synchronization is then required again after the computation to ensure all threads have finished with the data in shared memory before overwriting it in the next loop iteration.\n",
    "\n",
    "The function to synchronized threads is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9mI5kLpkJlP"
   },
   "source": [
    "```cuda.syncthreads()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nURLZ4AkNqu"
   },
   "source": [
    "This function will synchronize all threads in the same thread block. This function implements the same pattern as barriers in traditional multi-threaded programming and the ```MPI.Barrier()``` function. The program will wait until all threads in the block call the function, at which point it returns control to all its callers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rXRwx0IkcXZ"
   },
   "source": [
    "## Improved matrix multiplication\n",
    "The following example shows how shared memory can be used when performing matrix multiplication.\n",
    "\n",
    "In this example, each thread block is responsible for computing a square sub-matrix of C and each thread for computing an element of the sub-matrix. The sub-matrix is equal to the product of a square sub-matrix of A (sA) and a square sub-matrix of B (sB). In order to fit into the device resources, the two input matrices are divided into as many square sub-matrices of dimension TPB as necessary, and the result computed as the sum of the products of these square sub-matrices.\n",
    "\n",
    "Each product is performed by first loading sA and sB from global memory to shared memory, with one thread loading each element of each sub-matrix. Once sA and sB have been loaded, each thread accumulates the result into a register (tmp). Once all the products have been calculated, the results are written to the matrix C in global memory.\n",
    "\n",
    "By blocking the computation this way, we can reduce the number of global memory accesses since A is now only read B.shape[1] / TPB times and B is read A.shape[0] / TPB times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTeKG_RxlEWW"
   },
   "source": [
    "<img align=\"left\"  src=\"https://i.ibb.co/5xZ5cys/improve-matmul.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1677734884830,
     "user": {
      "displayName": "Krishna Kartik Darsipudi",
      "userId": "03239279300658474709"
     },
     "user_tz": 300
    },
    "id": "RdZm7qLGbNe8",
    "outputId": "0441133e-a037-4073-90d9-1eb0bf9ccf3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]]\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda, float32\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2) # 2 = 2D grid\n",
    "    print('hello')\n",
    "    print(x,y)\n",
    "    \n",
    "    tx = cuda.threadIdx.x # position of the thread in single block\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]     # ty + i * TPB is the global idx\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads() # wait to make sA, sB full before calculation\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "\n",
    "# The data array\n",
    "A = numpy.full((TPB*2, TPB*3), 3, float) # [32 x 48] matrix containing all 3's\n",
    "B = numpy.full((TPB*3, TPB*1), 4, float) # [48 x 16] matrix containing all 4's\n",
    "\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "C_global_mem = cuda.device_array((TPB*2, TPB*1)) # [32 x 16] matrix result\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[1]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[0]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "res = C_global_mem.copy_to_host()\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
