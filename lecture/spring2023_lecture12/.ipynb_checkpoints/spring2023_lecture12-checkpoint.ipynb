{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12\n",
    "## Introduction to GPUs (Graphics Processing Units)\n",
    "### Apr. 19, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of this lecture is based on the previous materials, see:\n",
    "\n",
    "https://nyu-cds.github.io/python-gpu/\n",
    "\n",
    "https://nyu-cds.github.io/python-numba/05-cuda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "A central processing unit (CPU) is designed to handle complex tasks, emulating virtual machines, control complex flows and branching, security, etc. In contrast, graphical processing units (GPUs) only do one thing well, namely, to handle billions of repetitive low level tasks.\n",
    "\n",
    "---\n",
    "\n",
    "Originally designed for the **rendering of triangles** in 3D graphics:\n",
    "[https://en.wikipedia.org/wiki/Triangle_mesh](https://en.wikipedia.org/wiki/Triangle_mesh)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**GPU**s have 1000s of **arithmetic logic units** (ALUs) compared with traditional CPUs that commonly have only 4 or 8. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Many types of scientific algorithms spend most of their time doing just what GPUs are good for: performing billions of repetitive arithmetic operations.\n",
    "\n",
    "\n",
    "<img src=\"cpugpuarch.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The following diagram shows how GPU performance has increased compared to traditional CPU architetures along the years.\n",
    "\n",
    "<img src=\"01-flops.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Difference between a CPU and a GPU\n",
    "This [video](https://www.youtube.com/watch?v=-P28LKWTzrI) is a funny illustration of the difference, in terms of processing capability, between CPUs and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "When computer scientists first attempted to use GPUs for scientific computing, the scientific codes had to be mapped onto operations designed to render triangles. This was incredibly difficult to do, and took a lot of time and dedication. \n",
    "\n",
    "---\n",
    "\n",
    "Nowadays, there are **high level languages** (such as **CUDA** and **OpenCL**) that target the GPUs directly, so GPU programming is rapidly becoming mainstream in the scientific community.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\"OpenCL is an open standard maintained by the non-profit technology consortium Khronos Group. Conformant implementations are available from Altera, AMD, Apple (OpenCL along with OpenGL is deprecated for Apple hardware, in favor of Metal), ARM, Creative, IBM, Imagination, Intel, Nvidia, Qualcomm, Samsung, Vivante, Xilinx, and ZiiLABS.\"\n",
    "\n",
    "CUDA is only implemented by Nvidia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **GPU program** comprises two parts: \n",
    "1. a *host part* that runs on the CPU: sets up the **parameters** and **data** for the computation\n",
    "2. one or more *kernels* that run on the GPU: perform the **actual computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "\n",
    "## CUDA Programming\n",
    "\n",
    "The CUDA parallel programming model has **three key abstractions** at its core:\n",
    "- a hierarchy of thread groups\n",
    "- shared memories\n",
    "- barrier synchronization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Granularity** in parallel programming: amount of computation vs communication.\n",
    "* **Fine-grained**: individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words.\n",
    "* **Coarse-grained**: data is communicated infrequently, after larger amounts of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The CUDA abstractions:\n",
    "* fine-grained data parallelism and thread parallelism (thread blocks)\n",
    "* coarse-grained data parallelism and task parallelism (grid)\n",
    "\n",
    "They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- A kernel is executed in parallel by an array of threads:\n",
    "    - All threads run the same code.\n",
    "    - Each thread has an ID that it uses to compute memory addresses and make control decisions.\n",
    "\n",
    "- Threads are arranged as a grid of thread blocks:\n",
    "    - Different grid/block can have different kernels  \n",
    "    - Threads from the same block have access to a shared memory and their execution can be synchronized\n",
    "\n",
    "<img src=\"threadgrid.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Thread blocks are required to execute independently: \n",
    "    - It must be possible to execute them in any order, in parallel or in series \n",
    "    - Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses.\n",
    "    - The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional.\n",
    "\n",
    "<img src=\"threadmapping.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CUDA is designed for a specific GPU architecture, namely NVIDIA’s Streaming Multiprocessors (SM). \n",
    "- Each SM has:\n",
    "    - a set of execution units\n",
    "    - a set of registers \n",
    "    - a chunk of shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"sm.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "In an NVIDIA GPU, the basic unit of execution is the __warp__. A warp is a collection of threads, 32 in current implementations, that are executed simultaneously by an SM. Multiple warps can be executed on an SM at once.\n",
    "\n",
    "When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to SMs with available execution capacity. The threads of a thread block execute concurrently on one SM, and multiple thread blocks can execute concurrently on one SM. As thread blocks terminate, new blocks are launched on the vacated SMs.\n",
    "\n",
    "The mapping between warps and thread blocks can affect the performance of the kernel. It is usually a good idea to keep the size of a thread block a multiple of 32 in order to avoid this as much as possible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thread Identity\n",
    "The index of a thread and its thread ID relate to each other as follows:\n",
    "- For a 1-dimensional block, the thread index and thread ID are the same\n",
    "- For a 2-dimensional block, the thread index (x,y) has thread ID=x+yDx, for block size (Dx,Dy)\n",
    "- For a 3-dimensional block, the thread index (x,y,x) has thread ID=x+yDx+zDxDy, for block size (Dx,Dy,Dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**When a kernel is started, the number of blocks per grid and the number of threads per block are fixed (gridDim and blockDim)**. CUDA makes four pieces of information available to each thread:\n",
    "- The thread index (threadIdx)\n",
    "- The block index (blockIdx)\n",
    "- The size and shape of a block (blockDim)\n",
    "- The size and shape of a grid (gridDim)\n",
    "\n",
    "Typically, each thread in a kernel will compute one element of an array. There is a common pattern to do this that most CUDA programs use are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CUDA simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pocl\n",
    "# !conda install oclgrind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have a CUDA-enabled GPU on your system, \n",
    "# you will receive one of the following errors:\n",
    "\n",
    "# numba.cuda.cudadrv.error.CudaDriverError: CUDA initialized before forking\n",
    "# CudaSupportError: Error at driver init: \n",
    "# [3] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:\n",
    "# numba.cuda.cudadrv.error.CudaDriverError: Error at driver init:\n",
    "# CUDA disabled by user:\n",
    "# If you do have a CUDA-enabled GPU on your system, you should see a message like:\n",
    "\n",
    "# <Managed Device 0>\n",
    "# If your machine has multiple GPUs, you might want to select which one to use. \n",
    "# By default the CUDA driver selects the fastest GPU as the device 0, \n",
    "# which is the default device used by Numba.\n",
    "\n",
    "# numba.cuda.select_device( device_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the CUDA simulator\n",
    "# If you don’t have a CUDA-enabled GPU \n",
    "# (i.e. you received one of the error messages described previously), \n",
    "# then you will need to use the CUDA simulator. \n",
    "# The simulator is enabled by setting the environment variable \n",
    "# NUMBA_ENABLE_CUDASIM to 1.\n",
    "\n",
    "\n",
    "# Mac/Linux\n",
    "# Launch a terminal shell and type the commands:\n",
    "!export NUMBA_ENABLE_CUDASIM=1\n",
    "\n",
    "# Windows\n",
    "# Launch a CMD shell and type the commands:\n",
    "# SET NUMBA_ENABLE_CUDASIM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NUMBA_ENABLE_CUDASIM=1\n"
     ]
    }
   ],
   "source": [
    "%env NUMBA_ENABLE_CUDASIM=1\n",
    "# set environment variables within jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)\n",
    "\n",
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i, t, b, w: 0 0 0 16\n",
      "i, t, b, w: 1 1 0 16\n",
      "i, t, b, w: 2 2 0 16\n",
      "i, t, b, w: 3 3 0 16\n",
      "i, t, b, w: 4 4 0 16\n",
      "i, t, b, w: 5 5 0 16\n",
      "i, t, b, w: 6 6 0 16\n",
      "i, t, b, w: 7 7 0 16\n",
      "i, t, b, w: 8 8 0 16\n",
      "i, t, b, w: 9 9 0 16\n",
      "i, t, b, w: 10 10 0 16\n",
      "i, t, b, w: 11 11 0 16\n",
      "i, t, b, w: 12 12 0 16\n",
      "i, t, b, w: 13 13 0 16\n",
      "i, t, b, w: 14 14 0 16\n",
      "i, t, b, w: 15 15 0 16\n",
      "i, t, b, w: 16 0 1 16\n",
      "i, t, b, w: 17 1 1 16\n",
      "i, t, b, w: 18 2 1 16\n",
      "i, t, b, w: 19 3 1 16\n",
      "i, t, b, w: 20 4 1 16\n",
      "i, t, b, w: 21 5 1 16\n",
      "i, t, b, w: 22 6 1 16\n",
      "i, t, b, w: 23 7 1 16\n",
      "i, t, b, w: 24 8 1 16\n",
      "i, t, b, w: 25 9 1 16\n",
      "i, t, b, w: 26 10 1 16\n",
      "i, t, b, w: 27 11 1 16\n",
      "i, t, b, w: 28 12 1 16\n",
      "i, t, b, w: 29 13 1 16\n",
      "i, t, b, w: 30 14 1 16\n",
      "i, t, b, w: 31 15 1 16\n",
      "i, t, b, w: 32 0 2 16\n",
      "i, t, b, w: 33 1 2 16\n",
      "i, t, b, w: 34 2 2 16\n",
      "i, t, b, w: 35 3 2 16\n",
      "i, t, b, w: 36 4 2 16\n",
      "i, t, b, w: 37 5 2 16\n",
      "i, t, b, w: 38 6 2 16\n",
      "i, t, b, w: 39 7 2 16\n",
      "i, t, b, w: 40 8 2 16\n",
      "i, t, b, w: 41 9 2 16\n",
      "i, t, b, w: 42 10 2 16\n",
      "i, t, b, w: 43 11 2 16\n",
      "i, t, b, w: 44 12 2 16\n",
      "i, t, b, w: 45 13 2 16\n",
      "i, t, b, w: 46 14 2 16\n",
      "i, t, b, w: 47 15 2 16\n",
      "i, t, b, w: 48 0 3 16\n",
      "i, t, b, w: 49 1 3 16\n",
      "i, t, b, w: 50 2 3 16\n",
      "i, t, b, w: 51 3 3 16\n",
      "i, t, b, w: 52 4 3 16\n",
      "i, t, b, w: 53 5 3 16\n",
      "i, t, b, w: 54 6 3 16\n",
      "i, t, b, w: 55 7 3 16\n",
      "i, t, b, w: 56 8 3 16\n",
      "i, t, b, w: 57 9 3 16\n",
      "i, t, b, w: 58 10 3 16\n",
      "i, t, b, w: 59 11 3 16\n",
      "i, t, b, w: 60 12 3 16\n",
      "i, t, b, w: 61 13 3 16\n",
      "i, t, b, w: 62 14 3 16\n",
      "i, t, b, w: 63 15 3 16\n",
      "i, t, b, w: 64 0 4 16\n",
      "i, t, b, w: 65 1 4 16\n",
      "i, t, b, w: 66 2 4 16\n",
      "i, t, b, w: 67 3 4 16\n",
      "i, t, b, w: 68 4 4 16\n",
      "i, t, b, w: 69 5 4 16\n",
      "i, t, b, w: 70 6 4 16\n",
      "i, t, b, w: 71 7 4 16\n",
      "i, t, b, w: 72 8 4 16\n",
      "i, t, b, w: 73 9 4 16\n",
      "i, t, b, w: 74 10 4 16\n",
      "i, t, b, w: 75 11 4 16\n",
      "i, t, b, w: 76 12 4 16\n",
      "i, t, b, w: 77 13 4 16\n",
      "i, t, b, w: 78 14 4 16\n",
      "i, t, b, w: 79 15 4 16\n",
      "i, t, b, w: 80 0 5 16\n",
      "i, t, b, w: 81 1 5 16\n",
      "i, t, b, w: 82 2 5 16\n",
      "i, t, b, w: 83 3 5 16\n",
      "i, t, b, w: 84 4 5 16\n",
      "i, t, b, w: 85 5 5 16\n",
      "i, t, b, w: 86 6 5 16\n",
      "i, t, b, w: 87 7 5 16\n",
      "i, t, b, w: 88 8 5 16\n",
      "i, t, b, w: 89 9 5 16\n",
      "i, t, b, w: 90 10 5 16\n",
      "i, t, b, w: 91 11 5 16\n",
      "i, t, b, w: 92 12 5 16\n",
      "i, t, b, w: 93 13 5 16\n",
      "i, t, b, w: 94 14 5 16\n",
      "i, t, b, w: 95 15 5 16\n",
      "i, t, b, w: 96 0 6 16\n",
      "i, t, b, w: 97 1 6 16\n",
      "i, t, b, w: 98 2 6 16\n",
      "i, t, b, w: 99 3 6 16\n",
      "i, t, b, w: 100 4 6 16\n",
      "i, t, b, w: 101 5 6 16\n",
      "i, t, b, w: 102 6 6 16\n",
      "i, t, b, w: 103 7 6 16\n",
      "i, t, b, w: 104 8 6 16\n",
      "i, t, b, w: 105 9 6 16\n",
      "i, t, b, w: 106 10 6 16\n",
      "i, t, b, w: 107 11 6 16\n",
      "i, t, b, w: 108 12 6 16\n",
      "i, t, b, w: 109 13 6 16\n",
      "i, t, b, w: 110 14 6 16\n",
      "i, t, b, w: 111 15 6 16\n",
      "i, t, b, w: 112 0 7 16\n",
      "i, t, b, w: 113 1 7 16\n",
      "i, t, b, w: 114 2 7 16\n",
      "i, t, b, w: 115 3 7 16\n",
      "i, t, b, w: 116 4 7 16\n",
      "i, t, b, w: 117 5 7 16\n",
      "i, t, b, w: 118 6 7 16\n",
      "i, t, b, w: 119 7 7 16\n",
      "i, t, b, w: 120 8 7 16\n",
      "i, t, b, w: 121 9 7 16\n",
      "i, t, b, w: 122 10 7 16\n",
      "i, t, b, w: 123 11 7 16\n",
      "i, t, b, w: 124 12 7 16\n",
      "i, t, b, w: 125 13 7 16\n",
      "i, t, b, w: 126 14 7 16\n",
      "i, t, b, w: 127 15 7 16\n",
      "i, t, b, w: 128 0 8 16\n",
      "i, t, b, w: 129 1 8 16\n",
      "i, t, b, w: 130 2 8 16\n",
      "i, t, b, w: 131 3 8 16\n",
      "i, t, b, w: 132 4 8 16\n",
      "i, t, b, w: 133 5 8 16\n",
      "i, t, b, w: 134 6 8 16\n",
      "i, t, b, w: 135 7 8 16\n",
      "i, t, b, w: 136 8 8 16\n",
      "i, t, b, w: 137 9 8 16\n",
      "i, t, b, w: 138 10 8 16\n",
      "i, t, b, w: 139 11 8 16\n",
      "i, t, b, w: 140 12 8 16\n",
      "i, t, b, w: 141 13 8 16\n",
      "i, t, b, w: 142 14 8 16\n",
      "i, t, b, w: 143 15 8 16\n",
      "i, t, b, w: 144 0 9 16\n",
      "i, t, b, w: 145 1 9 16\n",
      "i, t, b, w: 146 2 9 16\n",
      "i, t, b, w: 147 3 9 16\n",
      "i, t, b, w: 148 4 9 16\n",
      "i, t, b, w: 149 5 9 16\n",
      "i, t, b, w: 150 6 9 16\n",
      "i, t, b, w: 151 7 9 16\n",
      "i, t, b, w: 152 8 9 16\n",
      "i, t, b, w: 153 9 9 16\n",
      "i, t, b, w: 154 10 9 16\n",
      "i, t, b, w: 155 11 9 16\n",
      "i, t, b, w: 156 12 9 16\n",
      "i, t, b, w: 157 13 9 16\n",
      "i, t, b, w: 158 14 9 16\n",
      "i, t, b, w: 159 15 9 16\n",
      "i, t, b, w: 160 0 10 16\n",
      "i, t, b, w: 161 1 10 16\n",
      "i, t, b, w: 162 2 10 16\n",
      "i, t, b, w: 163 3 10 16\n",
      "i, t, b, w: 164 4 10 16\n",
      "i, t, b, w: 165 5 10 16\n",
      "i, t, b, w: 166 6 10 16\n",
      "i, t, b, w: 167 7 10 16\n",
      "i, t, b, w: 168 8 10 16\n",
      "i, t, b, w: 169 9 10 16\n",
      "i, t, b, w: 170 10 10 16\n",
      "i, t, b, w: 171 11 10 16\n",
      "i, t, b, w: 172 12 10 16\n",
      "i, t, b, w: 173 13 10 16\n",
      "i, t, b, w: 174 14 10 16\n",
      "i, t, b, w: 175 15 10 16\n",
      "i, t, b, w: 176 0 11 16\n",
      "i, t, b, w: 177 1 11 16\n",
      "i, t, b, w: 178 2 11 16\n",
      "i, t, b, w: 179 3 11 16\n",
      "i, t, b, w: 180 4 11 16\n",
      "i, t, b, w: 181 5 11 16\n",
      "i, t, b, w: 182 6 11 16\n",
      "i, t, b, w: 183 7 11 16\n",
      "i, t, b, w: 184 8 11 16\n",
      "i, t, b, w: 185 9 11 16\n",
      "i, t, b, w: 186 10 11 16\n",
      "i, t, b, w: 187 11 11 16\n",
      "i, t, b, w: 188 12 11 16\n",
      "i, t, b, w: 189 13 11 16\n",
      "i, t, b, w: 190 14 11 16\n",
      "i, t, b, w: 191 15 11 16\n",
      "i, t, b, w: 192 0 12 16\n",
      "i, t, b, w: 193 1 12 16\n",
      "i, t, b, w: 194 2 12 16\n",
      "i, t, b, w: 195 3 12 16\n",
      "i, t, b, w: 196 4 12 16\n",
      "i, t, b, w: 197 5 12 16\n",
      "i, t, b, w: 198 6 12 16\n",
      "i, t, b, w: 199 7 12 16\n",
      "i, t, b, w: 200 8 12 16\n",
      "i, t, b, w: 201 9 12 16\n",
      "i, t, b, w: 202 10 12 16\n",
      "i, t, b, w: 203 11 12 16\n",
      "i, t, b, w: 204 12 12 16\n",
      "i, t, b, w: 205 13 12 16\n",
      "i, t, b, w: 206 14 12 16\n",
      "i, t, b, w: 207 15 12 16\n",
      "i, t, b, w: 208 0 13 16\n",
      "i, t, b, w: 209 1 13 16\n",
      "i, t, b, w: 210 2 13 16\n",
      "i, t, b, w: 211 3 13 16\n",
      "i, t, b, w: 212 4 13 16\n",
      "i, t, b, w: 213 5 13 16\n",
      "i, t, b, w: 214 6 13 16\n",
      "i, t, b, w: 215 7 13 16\n",
      "i, t, b, w: 216 8 13 16\n",
      "i, t, b, w: 217 9 13 16\n",
      "i, t, b, w: 218 10 13 16\n",
      "i, t, b, w: 219 11 13 16\n",
      "i, t, b, w: 220 12 13 16\n",
      "i, t, b, w: 221 13 13 16\n",
      "i, t, b, w: 222 14 13 16\n",
      "i, t, b, w: 223 15 13 16\n",
      "i, t, b, w: 224 0 14 16\n",
      "i, t, b, w: 225 1 14 16\n",
      "i, t, b, w: 226 2 14 16\n",
      "i, t, b, w: 227 3 14 16\n",
      "i, t, b, w: 228 4 14 16\n",
      "i, t, b, w: 229 5 14 16\n",
      "i, t, b, w: 230 6 14 16\n",
      "i, t, b, w: 231 7 14 16\n",
      "i, t, b, w: 232 8 14 16\n",
      "i, t, b, w: 233 9 14 16\n",
      "i, t, b, w: 234 10 14 16\n",
      "i, t, b, w: 235 11 14 16\n",
      "i, t, b, w: 236 12 14 16\n",
      "i, t, b, w: 237 13 14 16\n",
      "i, t, b, w: 238 14 14 16\n",
      "i, t, b, w: 239 15 14 16\n",
      "i, t, b, w: 240 0 15 16\n",
      "i, t, b, w: 241 1 15 16\n",
      "i, t, b, w: 242 2 15 16\n",
      "i, t, b, w: 243 3 15 16\n",
      "i, t, b, w: 244 4 15 16\n",
      "i, t, b, w: 245 5 15 16\n",
      "i, t, b, w: 246 6 15 16\n",
      "i, t, b, w: 247 7 15 16\n",
      "i, t, b, w: 248 8 15 16\n",
      "i, t, b, w: 249 9 15 16\n",
      "i, t, b, w: 250 10 15 16\n",
      "i, t, b, w: 251 11 15 16\n",
      "i, t, b, w: 252 12 15 16\n",
      "i, t, b, w: 253 13 15 16\n",
      "i, t, b, w: 254 14 15 16\n",
      "i, t, b, w: 255 15 15 16\n",
      "\n",
      "data:\n",
      " [   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.  110.\n",
      "  120.  130.  140.  150.  160.  170.  180.  190.  200.  210.  220.  230.\n",
      "  240.  250.  260.  270.  280.  290.  300.  310.  320.  330.  340.  350.\n",
      "  360.  370.  380.  390.  400.  410.  420.  430.  440.  450.  460.  470.\n",
      "  480.  490.  500.  510.  520.  530.  540.  550.  560.  570.  580.  590.\n",
      "  600.  610.  620.  630.  640.  650.  660.  670.  680.  690.  700.  710.\n",
      "  720.  730.  740.  750.  760.  770.  780.  790.  800.  810.  820.  830.\n",
      "  840.  850.  860.  870.  880.  890.  900.  910.  920.  930.  940.  950.\n",
      "  960.  970.  980.  990. 1000. 1010. 1020. 1030. 1040. 1050. 1060. 1070.\n",
      " 1080. 1090. 1100. 1110. 1120. 1130. 1140. 1150. 1160. 1170. 1180. 1190.\n",
      " 1200. 1210. 1220. 1230. 1240. 1250. 1260. 1270. 1280. 1290. 1300. 1310.\n",
      " 1320. 1330. 1340. 1350. 1360. 1370. 1380. 1390. 1400. 1410. 1420. 1430.\n",
      " 1440. 1450. 1460. 1470. 1480. 1490. 1500. 1510. 1520. 1530. 1540. 1550.\n",
      " 1560. 1570. 1580. 1590. 1600. 1610. 1620. 1630. 1640. 1650. 1660. 1670.\n",
      " 1680. 1690. 1700. 1710. 1720. 1730. 1740. 1750. 1760. 1770. 1780. 1790.\n",
      " 1800. 1810. 1820. 1830. 1840. 1850. 1860. 1870. 1880. 1890. 1900. 1910.\n",
      " 1920. 1930. 1940. 1950. 1960. 1970. 1980. 1990. 2000. 2010. 2020. 2030.\n",
      " 2040. 2050. 2060. 2070. 2080. 2090. 2100. 2110. 2120. 2130. 2140. 2150.\n",
      " 2160. 2170. 2180. 2190. 2200. 2210. 2220. 2230. 2240. 2250. 2260. 2270.\n",
      " 2280. 2290. 2300. 2310. 2320. 2330. 2340. 2350. 2360. 2370. 2380. 2390.\n",
      " 2400. 2410. 2420. 2430. 2440. 2450. 2460. 2470. 2480. 2490. 2500. 2510.\n",
      " 2520. 2530. 2540. 2550.]\n"
     ]
    }
   ],
   "source": [
    "# %%writefile cuda01.py\n",
    "\n",
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    index = tx + bx * bw\n",
    "    io_array[index] = index * 10\n",
    "    print(\"i, t, b, w:\", index, tx, bx, bw)\n",
    "        \n",
    "        \n",
    "# Host code   \n",
    "data = numpy.ones(256)\n",
    "threadsperblock = 16\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(\"\\ndata:\\n\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### For a 2-dimensional grid:\n",
    "# tx = cuda.threadIdx.x\n",
    "# ty = cuda.threadIdx.y\n",
    "# bx = cuda.blockIdx.x\n",
    "# by = cuda.blockIdx.y\n",
    "# bw = cuda.blockDim.x\n",
    "# bh = cuda.blockDim.y\n",
    "# x = tx + bx * bw\n",
    "# y = ty + by * bh\n",
    "# array[x, y] = compute(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Hierarchy and Data Transfer\n",
    "The CPU and GPU have separate memory spaces. This means that data that is processed by the GPU must be moved from the CPU to the GPU before the computation starts, and the results of the computation must be moved back to the CPU once processing has completed.\n",
    "\n",
    "#### Global memory\n",
    "This memory is accessible to __all threads__ as well as the host (CPU).\n",
    "- Global memory is allocated and deallocated by the host\n",
    "- Used to initialize the data that the GPU will work on\n",
    "\n",
    "#### Shared memory\n",
    "__Each thread block__ has its own shared memory\n",
    "- Accessible only by threads within the block\n",
    "- Much faster than local or global memory\n",
    "- Requires special handling to get maximum performance\n",
    "- Only exists for the lifetime of the block\n",
    "\n",
    "#### Local memory\n",
    "__Each thread__ has its own private local memory\n",
    "- Only exists for the lifetime of the thread\n",
    "- Generally handled automatically by the compiler\n",
    "\n",
    "#### Constant and texture memory\n",
    "These are __read-only memory__ spaces accessible by __all threads__.\n",
    "- Constant memory is used to cache values that are shared by all functional units\n",
    "- Texture memory is optimized for texturing operations provided by the hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OpenCL and pyOpenCL\n",
    "__OpenCL__ (Open Computing Language) is an **open standard** for cross-platform, **parallel programming**. It was originally developed by Apple in 2008 and is now maintained by the Khronos Group.\n",
    "\n",
    "<img src=\"opencl.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    " \n",
    "While OpenCL supports many different types of processors, as for example GPUs, DSPs, and FPGAs, it is most notably used to access the GPU for general computing tasks.\n",
    "\n",
    "__pyOpenCL__ is a package (MIT license) that enables developers to easily access the OpenCL API from Python.\n",
    "\n",
    "A standard and a minimal OpenCL code will have following parts.\n",
    "1. Identifying a Platform\n",
    "2. Finding the device ID\n",
    "3. Creating the context: _to manage objects such as command-queues, memory, program and kernel objects and for executing kernels on one or more devices specified in the context._\n",
    "4. Creating a command queue in the context\n",
    "5. Creating a program source and a kernel entry point\n",
    "6. Creating the buffers for data handling\n",
    "7. Kernel Program\n",
    "8. Build and Launch the Kernel\n",
    "9. Read the Output Buffer and clear it (if needed)\n",
    "\n",
    "A _pyopencl_ user will have its own device identified by environment variables, simplifying things. Examples can be found [here](https://github.com/inducer/pyopencl/tree/master/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Important:**\n",
    "\n",
    "See and install pocl to get OpenCL device drivers: https://anaconda.org/conda-forge/pocl\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.1.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.3.1\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c conda-forge pocl -y\n",
    "# !pip install pyopencl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Platform - Name:  Apple\n",
      "Platform - Vendor:  Apple\n",
      "Platform - Version:  OpenCL 1.2 (Jun 17 2022 18:58:05)\n",
      "Platform - Profile:  FULL_PROFILE\n",
      "    ------\n",
      "    Device - Name:  Apple M1 Pro\n",
      "    Device - Type:  ALL | CPU\n",
      "    Device - Max Clock Speed:  2400 Mhz\n",
      "    Device - Compute Units:  8\n",
      "    Device - Local Memory:  32 KB\n",
      "    Device - Constant Memory:  64 KB\n",
      "    Device - Global Memory: 16 GB\n",
      "    ------\n",
      "    Device - Name:  Apple M1 Pro\n",
      "    Device - Type:  ALL | GPU\n",
      "    Device - Max Clock Speed:  1000 Mhz\n",
      "    Device - Compute Units:  14\n",
      "    Device - Local Memory:  32 KB\n",
      "    Device - Constant Memory:  1048576 KB\n",
      "    Device - Global Memory: 11 GB\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%writefile info.py\n",
    "\n",
    "# Find out about your computer's OpenCL situation\n",
    "import pyopencl as cl  # Import the OpenCL GPU computing API\n",
    "\n",
    "for platform in cl.get_platforms():  # Print each platfor m on this computer\n",
    "    print('=' * 10)\n",
    "    print('Platform - Name:  ' + platform.name)\n",
    "    print('Platform - Vendor:  ' + platform.vendor)\n",
    "    print('Platform - Version:  ' + platform.version)\n",
    "    print('Platform - Profile:  ' + platform.profile)\n",
    "    \n",
    "    for device in platform.get_devices():  # Print each device per-platform\n",
    "        print('    ' + '-' * 6)\n",
    "        print('    Device - Name:  ' + device.name)\n",
    "        print('    Device - Type:  ' + cl.device_type.to_string(device.type))\n",
    "        print('    Device - Max Clock Speed:  {0} Mhz'.format(device.max_clock_frequency))\n",
    "        print('    Device - Compute Units:  {0}'.format(device.max_compute_units))\n",
    "        print('    Device - Local Memory:  {0:.0f} KB'.format(device.local_mem_size/1024))\n",
    "        print('    Device - Constant Memory:  {0:.0f} KB'.format(device.max_constant_buffer_size/1024))\n",
    "        print('    Device - Global Memory: {0:.0f} GB'.format(device.global_mem_size/1073741824.0))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyopencl.Context at 0x60000152c000 on <pyopencl.Device 'Apple M1 Pro' on 'Apple' at 0xffffffff>>\n",
      "Took 0.5153937339782715s\n",
      "a: [0.44207773 0.12564729 0.41129044 ... 0.9620654  0.36027578 0.1719678 ]\n",
      "b: [0.33604944 0.14279251 0.71317345 ... 0.17485932 0.5438025  0.37021676]\n",
      "c: [0.7781272  0.2684398  1.1244639  ... 1.1369247  0.90407825 0.5421846 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use OpenCL To Add Two Random Arrays (This Way Hides Details)\n",
    "from time import time\n",
    "import pyopencl as cl  # Import the OpenCL GPU computing API\n",
    "import pyopencl.array as pycl_array  # Import PyOpenCL Array \n",
    "#(a Numpy array plus an OpenCL buffer object)\n",
    "\n",
    "import numpy as np  # Import Numpy number tools\n",
    "\n",
    "# platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "# device = platform.get_devices()[0]  # Select the first device on this platform [0]\n",
    "# context = cl.Context([device])  # Create a context with your device\n",
    "context = cl.create_some_context()  # Initialize the Context\n",
    "\n",
    "print(context)\n",
    "queue   = cl.CommandQueue(context)  # Instantiate a Queue\n",
    "\n",
    "# Create two random pyopencl arrays\n",
    "a = pycl_array.to_device(queue, np.random.rand(50000).astype(np.float32))\n",
    "b = pycl_array.to_device(queue, np.random.rand(50000).astype(np.float32))  \n",
    "\n",
    "# Create an empty pyopencl destination array\n",
    "ts = time()\n",
    "res_c = pycl_array.empty_like(a)  \n",
    "\n",
    "program = cl.Program(context, \"\"\"\n",
    "__kernel void sum(__global const float *a, __global const float *b, __global float *c)\n",
    "{\n",
    "  int i = get_global_id(0);\n",
    "  c[i] = a[i] + b[i];\n",
    "}\"\"\").build()  # Create the OpenCL program\n",
    "\n",
    "# Enqueue the program for execution and store the result in c\n",
    "program.sum(queue, a.shape, None, a.data, b.data, res_c.data)  \n",
    "print('Took {}s'.format(time() - ts))\n",
    "\n",
    "print(\"a: {}\".format(a))\n",
    "print(\"b: {}\".format(b))\n",
    "print(\"c: {}\".format(res_c))  \n",
    "# Print all three arrays, to show sum() worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example at PyOpenCL's documentation: https://documen.tician.de/pyopencl/\n",
    "\n",
    "\n",
    "See [https://documen.tician.de/pyopencl/runtime_program.html](https://documen.tician.de/pyopencl/runtime_program.html)\n",
    "\n",
    "and *associated memory object* `mem_info`: \n",
    "[https://documen.tician.de/pyopencl/runtime_const.html#mem_info](https://documen.tician.de/pyopencl/runtime_const.html#mem_info)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyopencl.Device 'Apple M1 Pro' on 'Apple' at 0xffffffff>,\n",
       " <pyopencl.Device 'Apple M1 Pro' on 'Apple' at 0x1027f00>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platform.get_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyopencl.Context at 0x60000103c9a0 on <pyopencl.Device 'Apple M1 Pro' on 'Apple' at 0x1027f00>>\n",
      "Took 0.06413006782531738s\n",
      "[0.31553864 0.21231516 0.63342094 0.22755887 0.7649197  0.21828581\n",
      " 0.24414408 0.08285043 0.15933816 0.2928353 ]\n",
      "[0.8402543  0.23760873 0.6133714  0.5319221  0.84470075 0.7371961\n",
      " 0.4617338  0.1672485  0.4165711  0.18250176]\n",
      "[1.155793   0.44992387 1.2467923  0.75948095 1.6096205  0.9554819\n",
      " 0.7058779  0.25009894 0.57590926 0.47533706]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the same above algorithm but written in a different way\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "\n",
    "n = 5_000_000\n",
    "\n",
    "a_np = np.random.rand(n).astype(np.float32)\n",
    "b_np = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "# ctx = cl.create_some_context()\n",
    "platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "device = platform.get_devices()[1]  # Select the first device on this platform [0]\n",
    "ctx = cl.Context([device])  # Create a context with your device\n",
    "print(ctx)\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "# Buffer: class pyopencl.Buffer(context, flags, size=0, hostbuf=None)\n",
    "\n",
    "mf = cl.mem_flags\n",
    "a_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "b_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b_np)\n",
    "\n",
    "\n",
    "# get_global_id\n",
    "# Returns the unique global work-item ID value \n",
    "# for dimension identified by dimindx.\n",
    "\n",
    "prg = cl.Program(ctx, \"\"\"\n",
    "__kernel void sum(\n",
    "    __global const float *a_g, __global const float *b_g, __global float *res_g)\n",
    "{\n",
    "  int gid = get_global_id(0);\n",
    "  res_g[gid] = a_g[gid] + b_g[gid];\n",
    "}\n",
    "\"\"\").build()\n",
    "\n",
    "ts = time()\n",
    "\n",
    "res_g = cl.Buffer(ctx, mf.WRITE_ONLY, a_np.nbytes)\n",
    "prg.sum(queue, a_np.shape, None, a_g, b_g, res_g)\n",
    "\n",
    "res_np = np.empty_like(a_np)\n",
    "cl.enqueue_copy(queue, res_np, res_g)\n",
    "\n",
    "print('Took {}s'.format(time() - ts))\n",
    "\n",
    "# Check on CPU with Numpy:\n",
    "print(a_np[0: 10])\n",
    "print(b_np[0: 10])\n",
    "print(res_np[0: 10])\n",
    "\n",
    "print((res_np - (a_np + b_np))[0:10])\n",
    "print(np.linalg.norm(res_np - (a_np + b_np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "pyOpenCL has two goals:\n",
    "- Make OpenCL seem simple\n",
    "- Expose OpenCL's complex features\n",
    "\n",
    "Comparing the two previous codes we see that\n",
    "```python\n",
    "context = cl.create_some_context()\n",
    "```\n",
    "is simple, but if you have two GPUs in your computer, this function might select the wrong one.  Therefore, you might want to write three lines instead of one:\n",
    "```python\n",
    "platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "device = platform.get_devices()[0]  # Select the first device on this platform [0]\n",
    "context = cl.Context([device])  # Create a context with your device\n",
    "```\n",
    "This second way of creating a context is longer, but it allows you to target the exact platform and device you want to use on your machine."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
